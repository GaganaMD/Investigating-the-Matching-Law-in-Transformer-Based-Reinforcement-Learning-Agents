# -*- coding: utf-8 -*-
"""Experiment 1 Transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EzUG7PRG497IKpn5v3gkfb_XiP6y-Pe-
"""

pip install torch torchvision gym matplotlib numpy stable-baselines3 shimmy

"""## Using two digits while varying reward for one"""

import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import matplotlib.pyplot as plt
import torchvision
import torchvision.transforms as transforms
from gym import spaces
from torch.utils.data import DataLoader, Subset
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.policies import ActorCriticPolicy
import pandas as pd

class MNISTSequenceEnv(gym.Env):
    def __init__(self, reward_map={2: 10, 5: 1}, penalty=-1):
        super(MNISTSequenceEnv, self).__init__()

        # multiple numbers with different reward values
        self.reward_map = reward_map
        self.penalty = penalty

        transform = transforms.Compose([transforms.ToTensor()])
        self.mnist_full = torchvision.datasets.MNIST(root='./data', train=True,
                                              download=True, transform=transform)

        # Filter for only 2s and 5s
        indices = [(i, label) for i, (_, label) in enumerate(self.mnist_full) if label in [2, 5]]
        self.indices = [i for i, _ in indices]
        self.mnist = Subset(self.mnist_full, self.indices)

        self.observation_space = spaces.Box(low=0, high=1,
                                          shape=(10, 28, 28), dtype=np.float32)
        self.action_space = spaces.Discrete(2)

        # Mapping between action and actual digit
        self.action_to_digit = {0: 2, 1: 5}
        self.digit_to_action = {2: 0, 5: 1}

        # Initialize counters
        self.reset_counters()

        # Track total steps
        self.total_steps = 0
        self.episode_count = 0

    def reset_counters(self):
        self.response_counts = {2: 0, 5: 0}  # counts how many times each digit was chosen
        self.reward_values = self.reward_map.copy()  # the actual reward values for each digit

    def step(self, action):
        # Convert action (0/1) to actual digit (2/5)
        digit = self.action_to_digit[action]

        # Track the agent's choice
        self.response_counts[digit] += 1
        self.total_steps += 1

        # Always give the reward associated with the chosen digit
        reward = self.reward_map[digit]

        # Reset if needed - track episodes
        done = True
        if done:
            self.episode_count += 1

        return np.array(self.sequence, dtype=np.float32), reward, done, {}

    def reset(self):
        self.sequence = []
        self.labels = []

        for _ in range(5):
            img, label = random.choice(self.mnist)
            self.sequence.append(img.squeeze(0).numpy())
            self.sequence.append(np.zeros((28, 28)))
            self.labels.append(label)

        return np.array(self.sequence, dtype=np.float32)

    def get_matching_analysis(self):
        # response and reward ratios for pairs of digits
        digits = list(self.reward_map.keys())
        ratios = {}
        for i in range(len(digits)):
            for j in range(i+1, len(digits)):
                d1, d2 = digits[i], digits[j]
                response_ratio = self.response_counts[d1] / max(1, self.response_counts[d2])
                reward_ratio = self.reward_values[d1] / self.reward_values[d2]
                ratios[f'{d1}vs{d2}'] = {
                    'response_ratio': response_ratio,
                    'reward_ratio': reward_ratio,
                    'response_counts': {
                        d1: self.response_counts[d1],
                        d2: self.response_counts[d2]
                    }
                }
        return ratios

class TransformerRLAgent(nn.Module):
    def __init__(self, num_classes=10):
        super(TransformerRLAgent, self).__init__()

        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)

        self.embedding = nn.Linear(128, 64)
        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)

        self.fc2 = nn.Linear(64, num_classes)

    def forward(self, x):
        batch_size, seq_len, h, w = x.shape
        x = x.view(batch_size * seq_len, 1, h, w)

        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(batch_size * seq_len, -1)
        x = self.fc1(x)

        x = x.view(batch_size, seq_len, -1)
        x = self.embedding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)

        return self.fc2(x)

# Custom feature extractor for stable-baselines3 that uses the transformer
class TransformerFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=64, num_classes=10):
        super(TransformerFeatureExtractor, self).__init__(observation_space, features_dim)

        # Use the transformer agent up to the transformer output
        self.transformer_agent = TransformerRLAgent(num_classes=num_classes)

        # Override the features_dim with the output size of the transformer
        self._features_dim = features_dim

        # Create an additional layer to match the expected features_dim
        self.features_reshape = nn.Linear(64, features_dim)

    def forward(self, observations):
        # Process through the transformer (except final layer)
        batch_size, seq_len, h, w = observations.shape
        x = observations.view(batch_size * seq_len, 1, h, w)

        x = F.relu(self.transformer_agent.conv1(x))
        x = F.relu(self.transformer_agent.conv2(x))
        x = x.view(batch_size * seq_len, -1)
        x = self.transformer_agent.fc1(x)

        x = x.view(batch_size, seq_len, -1)
        x = self.transformer_agent.embedding(x)
        x = self.transformer_agent.transformer(x)
        x = x.mean(dim=1)

        # Reshape to the expected features_dim if necessary
        if self._features_dim != 64:
            x = self.features_reshape(x)

        return x

# Custom policy that uses the transformer feature extractor
class TransformerPolicy(ActorCriticPolicy):
    def __init__(
        self,
        observation_space,
        action_space,
        lr_schedule,
        net_arch=None,
        activation_fn=nn.Tanh,
        *args,
        **kwargs,
    ):
        # Default architecture with separate networks
        if net_arch is None:
            net_arch = [dict(pi=[64, 64], vf=[64, 64])]

        # Create a custom feature extractor
        features_extractor_class = TransformerFeatureExtractor
        features_extractor_kwargs = dict(features_dim=64, num_classes=action_space.n)

        super(TransformerPolicy, self).__init__(
            observation_space,
            action_space,
            lr_schedule,
            net_arch,
            activation_fn,
            features_extractor_class=features_extractor_class,
            features_extractor_kwargs=features_extractor_kwargs,
            *args,
            **kwargs,
        )

class RewardCallback(BaseCallback):
    # for tracking rewards
    def __init__(self, verbose=0):
        super(RewardCallback, self).__init__(verbose)
        self.rewards = []
        self.episode_rewards = []
        self.cumulative_reward = 0
        self.step_counter = 0
        self.window_size = 100  # For moving average

    def _on_step(self):
        # Get info from the last step
        reward = self.locals['rewards'][0]  # Assuming single env
        self.cumulative_reward += reward

        # Record reward
        self.rewards.append(reward)

        # Record running stats every window_size steps
        self.step_counter += 1
        if self.step_counter % self.window_size == 0:
            avg_reward = sum(self.rewards[-self.window_size:]) / self.window_size
            self.episode_rewards.append({
                'step': self.step_counter,
                'avg_reward': avg_reward,
                'cumulative_reward': self.cumulative_reward
            })

        return True

def run_experiment(reward_values_digit2, fixed_reward_digit1=1, runs_per_value=10, training_steps=5000):
    results = []
    reward_histories = {}

    for reward_value in reward_values_digit2:
        print(f"Testing reward value {reward_value} for digit 2 (vs {fixed_reward_digit1} for digit 5)")
        reward_histories[reward_value] = []

        for run in range(runs_per_value):
            if run % 10 == 0:
                print(f"  Run {run}/{runs_per_value}")

            # Create environment with the current reward configuration
            env = MNISTSequenceEnv(reward_map={2: reward_value, 5: fixed_reward_digit1})

            # Create a callback to track rewards
            reward_callback = RewardCallback()

            # Create and train the agent with the custom transformer policy
            model = PPO(
                TransformerPolicy,
                env,
                verbose=0,
                policy_kwargs=dict(
                    net_arch=[dict(pi=[64, 64], vf=[64, 64])],
                )
            )

            model.learn(total_timesteps=training_steps, callback=reward_callback)

            # Store the reward history for this run
            reward_histories[reward_value].append({
                'run': run,
                'rewards': reward_callback.rewards,
                'episode_rewards': reward_callback.episode_rewards,
                'cumulative_reward': reward_callback.cumulative_reward
            })

            # Get matching analysis
            analysis = env.get_matching_analysis()
            pair_key = '2vs5'  # The key for the digit pair we're interested in

            # Record results
            results.append({
                'reward_value_digit2': reward_value,
                'reward_value_digit5': fixed_reward_digit1,
                'reward_ratio': analysis[pair_key]['reward_ratio'],
                'response_ratio': analysis[pair_key]['response_ratio'],
                'selection_rate_digit2': env.response_counts[2] / max(1, (env.response_counts[2] + env.response_counts[5])),
                'selection_rate_digit5': env.response_counts[5] / max(1, (env.response_counts[2] + env.response_counts[5])),
                'counts_digit2': env.response_counts[2],
                'counts_digit5': env.response_counts[5],
                'total_episodes': env.episode_count,
                'total_reward': reward_callback.cumulative_reward,
                'run': run
            })

    return pd.DataFrame(results), reward_histories

def plot_results(results_df, reward_histories=None):
    # Aggregate results by reward value
    grouped = results_df.groupby('reward_value_digit2').agg({
        'selection_rate_digit2': ['mean', 'std'],
        'reward_ratio': 'mean',
        'total_reward': 'mean'
    }).reset_index()

    # Rename columns for easier access
    grouped.columns = ['reward_value_digit2', 'selection_rate_mean', 'selection_rate_std',
                      'reward_ratio', 'avg_total_reward']

    # Create figure
    plt.figure(figsize=(12, 12))

    # Plot selection rate vs reward value
    plt.subplot(3, 1, 1)
    plt.errorbar(grouped['reward_value_digit2'], grouped['selection_rate_mean'],
                 yerr=grouped['selection_rate_std'], fmt='o-', capsize=5)
    plt.xlabel('Reward Value for Digit 2')
    plt.ylabel('Selection Rate for Digit 2')
    plt.title('Effect of Reward Amount on Selection Rate')
    plt.grid(True)

    # Plot average total reward by reward value
    plt.subplot(3, 1, 2)
    plt.bar(grouped['reward_value_digit2'], grouped['avg_total_reward'])
    plt.xlabel('Reward Value for Digit 2')
    plt.ylabel('Average Total Reward')
    plt.title('Total Reward Accumulated by Reward Value')
    plt.grid(True)

    # Plot selection rate vs reward ratio (log-log scale for matching law)
    plt.subplot(3, 1, 3)

    # Convert selection rates to response ratios for the log-log plot
    response_ratios = grouped['selection_rate_mean'] / (1 - grouped['selection_rate_mean'])

    plt.loglog(grouped['reward_ratio'], response_ratios, 'o-')

    # Add ideal matching law line (y = x)
    min_ratio = grouped['reward_ratio'].min()
    max_ratio = grouped['reward_ratio'].max()
    ideal_line = np.linspace(min_ratio, max_ratio, 100)
    plt.loglog(ideal_line, ideal_line, 'k--', label='Ideal Matching')

    plt.xlabel('Reward Ratio (Digit 2 / Digit 5)')
    plt.ylabel('Response Ratio (Digit 2 / Digit 5)')
    plt.title('Matching Law: Log-Log Plot of Response Ratio vs Reward Ratio')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('matching_law_results.png')
    plt.show()

    # Plot learning curves
    if reward_histories:
        plt.figure(figsize=(12, 8))

        # Plot cumulative reward over time for first run of each reward value
        for reward_value, histories in reward_histories.items():
            if histories:  # Check if we have data
                # Get the first run's data
                first_run = histories[0]

                # Extract step numbers and rewards from episode_rewards
                steps = [entry['step'] for entry in first_run['episode_rewards']]
                rewards = [entry['avg_reward'] for entry in first_run['episode_rewards']]
                cumulative = [entry['cumulative_reward'] for entry in first_run['episode_rewards']]

                # Plot moving average reward
                plt.subplot(2, 1, 1)
                plt.plot(steps, rewards, label=f'Reward Value {reward_value}')

                # Plot cumulative reward
                plt.subplot(2, 1, 2)
                plt.plot(steps, cumulative, label=f'Reward Value {reward_value}')

        plt.subplot(2, 1, 1)
        plt.xlabel('Training Steps')
        plt.ylabel('Average Reward (Moving Window)')
        plt.title('Learning Progress: Average Reward Over Time')
        plt.legend()
        plt.grid(True)

        plt.subplot(2, 1, 2)
        plt.xlabel('Training Steps')
        plt.ylabel('Cumulative Reward')
        plt.title('Learning Progress: Cumulative Reward Over Time')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('learning_curves.png')
        plt.show()

# Main experiment
if __name__ == "__main__":
    # Define reward values to test for digit 2 (digit 5 reward is fixed at 1)
    reward_values = [1, 2, 4, 8, 16]

    # Run experiment with x runs per reward value
    results, reward_histories = run_experiment(
        reward_values_digit2=reward_values,
        fixed_reward_digit1=1,
        runs_per_value=5,  # Reduced for faster execution
        training_steps=5000
    )

    # Save results to CSV
    results.to_csv('matching_law_results.csv', index=False)

    # Plot results including reward histories
    plot_results(results, reward_histories)

    # Print summary statistics
    print("\nSummary Statistics:")
    summary = results.groupby('reward_value_digit2').agg({
        'selection_rate_digit2': ['mean', 'std', 'count'],
        'reward_ratio': 'mean',
        'response_ratio': 'mean',
        'total_reward': ['mean', 'std']
    })
    print(summary)

    # Print learning progress for first run of each reward value
    print("\nLearning Progress (First Run of Each Reward Value):")
    for reward_value, histories in reward_histories.items():
        if histories:
            first_run = histories[0]
            print(f"\nReward Value {reward_value}:")
            print(f"  Total Reward: {first_run['cumulative_reward']}")

            # Optionally print some milestone points to show progress
            episode_rewards = first_run['episode_rewards']
            if episode_rewards:
                milestones = [0, len(episode_rewards)//4, len(episode_rewards)//2, 3*len(episode_rewards)//4, -1]
                print("  Learning milestones (steps, avg reward, cumulative):")
                for i in milestones:
                    if i >= 0 and i < len(episode_rewards):
                        entry = episode_rewards[i]
                        print(f"    Step {entry['step']}: Avg reward = {entry['avg_reward']:.2f}, " +
                              f"Cumulative = {entry['cumulative_reward']:.2f}")